---
title: "Pretty Figures for Pub"
author: "Jesse Granger"
date: "March 29, 2019"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
#!diagnostics off
knitr::opts_chunk$set(echo = TRUE)
setwd("C:/Users/Jesse/Dropbox/Jesse/Duke")
source("Research/Navigation Orientation/Whales/Strandings/Paper Draft/CurrentBio/Final Submission/To Submit/Functions.R")
library(readxl)
library(lunar)
library(anytime)
nulldata=read_excel("Research/Navigation Orientation/Whales/Strandings/Raw Data/WeeklyMagneticdata__total.xlsx")
nulldata=completeFun(nulldata,"MaxAP")
nulldata=nulldata[which(nulldata[,"Year"]>1985),]

allgray=read_excel("Research/Navigation Orientation/Whales/Strandings/Raw Data/LiveDataUse.gray.week.xlsx")
allgray=allgray[1:186,]
allgray=completeFun(allgray, "RadioFluxLA")

```

#Plots of the data, not included in the paper but discussed qualitatively

###Day-of-year of whales migration

```{r}
#Plot the data using a day-of-year metric, where 1 is January first, and 365/366 is December 31st (depending on whether it is a leap year)
hist(allgray$DateAsCont, breaks=50, main="Histogram of strandings by day-of-year", xlab="Day of Year", xlim=c(0,400), ylim=c(0,15))
#This is a way to do the same thing but see which months whales are stranding more often on
#hist(allgray$Month, xlim=c(0,13))
```
###Strandings by year

```{r}
#Make a histogram of the data by year

hist(allgray$Year,breaks=40, xlim=c(1985,2020), xlab="Year", main="Live Gray Strandings by Year",col=adjustcolor("gray",alpha.f=0.5), 
     cex.axis=1.2,
     cex.lab=1.5, cex.main=1.5)
```
###Spatial mapping

```{r}
#Show where along the coast whales are stranding

library(maps)
#Make a plot of the U.S. coast, including Alaska, but trim to the desired lat. and long.
map("world", c("USA", "hawaii"), xlim = c(-180, -110), ylim = c(30, 80),
    main="All US Strandings")
#Add the stranding data as points by their lat. and long.
points(allgray$Longitude, allgray$Latitude, pch=".", cex=5, col= "red")
```


#Figure 1: A-C

###For loop to caluclate Bins

```{r, echo=FALSE}
#This section runs a for loop which bins the data, and repeats it n times in order to get the deviation caused by randomly #sampling from the null data

nulldata.RF=completeFun(nulldata,"RadioFluxLA") #Remove NAs from the nulldata for the RF column
n=10000  #Number of iterations of bootstrap

#Initiate variables 
##data frames to hold the result for each iteration of the bins
SSbins_Max=setNames(data.frame(matrix(ncol = 6, nrow = 0)), c("0-12", "12-27", "27-56", "56-95", "95-151", "151+"))
RFbins=setNames(data.frame(matrix(ncol = 6, nrow = 0)), c("0-72", "72-82", "82-100", "100-126", "126-165", "165+"))
Kbins=setNames(data.frame(matrix(ncol=6,nrow=0)), c("1-9","9-15","15-23","23-35","35-54","54+"))

#Open a progress bar
pb <- winProgressBar(title = "progress bar", min = 0,
                     max = n, width = 300)

for (i in seq(1,n)) {
  
  #Create data in the desired format
  datause=create.bootstrap(allgray,nulldata.RF)
  
  #Bin and save group proportions
 
  SSbins_Max[i,]=binSS(datause)
  RFbins[i,]=binRF(datause)
  Kbins[i,]=binK(datause)
  
  #Update progress bar
  setWinProgressBar(pb, i, title=paste( round(i/n*100, 0),
                                        "% done"))
}
#Close progress bar
close(pb)

```

###Plot the bins with error bars

```{r}
  #Bin sunspots

#Remove 0's or infinite columns
  SSbins_Max<- SSbins_Max[is.finite(rowSums(SSbins_Max)),]
#Find and save the mean for each bin
  means=c(mean(SSbins_Max$`0-12`), mean(SSbins_Max$`12-27`), mean(SSbins_Max$`27-56`), mean(SSbins_Max$`56-95`), mean(SSbins_Max$`95-151`),mean(SSbins_Max$`151+`))
#Find and save the standard deviation for each bin  
  se=c(sd(SSbins_Max$`0-12`), sd(SSbins_Max$`12-27`), sd(SSbins_Max$`27-56`), sd(SSbins_Max$`56-95`), sd(SSbins_Max$`95-151`),sd(SSbins_Max$`151+`))
#Make a variable that will be the top of the error bars  
  plotTop=max(means+se*2)
#Name of each bin  
  names.SS=c("0 - 12", "12 - 27", "27 - 65", "56 - 95", "95 - 151", "151+")
#Plot the mean of each bin with desired labels  
barCenters <- barplot(means,
                      col="gray", las=1, ylim=c(0,plotTop+.1),
                      main = "A",
                      cex.axis =1.2,
                      cex.lab=1.5,
                      ylab = "Ratio live:null")
mtext("SunSpot Count", side=1, line=4, cex = 1.3)
text(barCenters,par("usr")[3] - 0.12,labels=names.SS,srt=45,pos=1,xpd=TRUE, cex=1.2)
#Add the error bars
arrows(barCenters, means-se*2, barCenters, means+se*2,
       lwd=2, angle=90, code=3, length = .1)

  #BinRF
#Repeat all code from above for RF
RFbins<- RFbins[is.finite(rowSums(RFbins)),]
  means=c(mean(RFbins$`0-72`), mean(RFbins$`72-82`), mean(RFbins$`82-100`), mean(RFbins$`100-126`), mean(RFbins$`126-165`),mean(RFbins$`165+`))
  se=c(sd(RFbins$`0-72`), sd(RFbins$`72-82`), sd(RFbins$`82-100`), sd(RFbins$`100-126`), sd(RFbins$`126-165`),sd(RFbins$`165+`))
  plotTop=max(means+se*2)
  names.RF=c("0 - 72","72 - 82","82 - 100","100 - 126","126 - 165","165+")
barCenters <- barplot(means,
                      col="gray", las=1, ylim=c(0,plotTop+.1),
                      main = "B",
                      cex.axis =1.2,
                      cex.lab=1.5,
                      ylab = "Ratio live:null")
mtext("RF Index", side=1, line=4, cex = 1.3)
text(barCenters,par("usr")[3] - 0.31,labels=names.RF,srt=45,pos=1,xpd=TRUE, cex=1.2)
arrows(barCenters, means-se*2, barCenters, means+se*2,
       lwd=2, angle=90, code=3, length = .1)

#Bin Ap
#Repeat all code from above for Ap (labeled K-here)
Kbins<- Kbins[is.finite(rowSums(Kbins)),]
means=c(mean(Kbins$`1-9`), mean(Kbins$`9-15`), mean(Kbins$`15-23`), mean(Kbins$`23-35`), mean(Kbins$`35-54`),mean(Kbins$`54+`))
se=c(sd(Kbins$`1-9`), sd(Kbins$`9-15`), sd(Kbins$`15-23`), sd(Kbins$`23-35`), sd(Kbins$`35-54`),sd(Kbins$`54+`))

  plotTop=max(means+se*2)
  names.K=c("1-9","9-15","15-23","23-35","35-54","54+")
barCenters <- barplot(means,
                      col="gray", las=1, ylim=c(0,plotTop+.1),
                      main = "C",
                      cex.axis =1.2,
                      cex.lab=1.5,
                      ylab = "Ratio live:null")
mtext("Ap Index", side=1, line=4, cex = 1.3)
text(barCenters,par("usr")[3] - 0.1,labels=names.K,srt=45,pos=1,xpd=TRUE, cex=1.2)
arrows(barCenters, means-se*2, barCenters, means+se*2,
       lwd=2, angle=90, code=3, length = .1)
```

#Figure 1 D-E: null distibutions compared to mean of data+p-value
###For Loop to create the underlying distribution
```{r}
#This is a for loop to create the underlying distribution of our data

##Variables were standardized using a z-transformation, where you subtract the mean and divide by the standard deviation of your variable. The result will have mean=0 and sd=1. An example is included below
#nulldata.RF$RF.scale <- (nulldata.RF$RadioFluxLA - mean(nulldata.RF$RadioFluxLA)) / sd(nulldata.RF$RadioFluxLA)

#Initiate variables 
n=10000 #Number of times to iterate

##Variables to save the mean SS, RF, or Ap value from each sample
xbar.n.SS=rep(0,n)
xbar.n.RF=rep(0,n)
xbar.n.k=rep(0,n)

#Only draw data from the seasons whales are most often migrating
nulldata.RF.season=nulldata.RF[nulldata.RF$DateAsCont>50,]
nulldata.RF.season=nulldata.RF.season[nulldata.RF.season$DateAsCont<200,]

#Open a progress bar
pb <- winProgressBar(title = "progress bar", min = 0,
                     max = n, width = 300)

for (i in seq(1,n)) {
  
  #Take a sample from the null data, of the same size as the stranding dataframe
  datause=nulldata.RF.season[sample(nrow(nulldata.RF.season),185, replace=TRUE ),]
  #Save the means of each dist
  xbar.n.SS[i]=mean(datause$SS.scale)
  xbar.n.RF[i]=mean(datause$RF.scale)
  xbar.n.k[i]=mean(datause$AP.scale)
  
  #Update progress bar
  setWinProgressBar(pb, i, title=paste( round(i/n*100, 0),"% done"))
}
#Close progress bar
close(pb)
```

###Plot the underlying distribution compared to the mean of the data, and calcualate the p-value
```{r}
#Remove any NAs from the stranding data
allgray=completeFun(allgray,"RF.scale")


#Plot the stranding underlying distribution with desired labels
hist(xbar.n.SS, 
     breaks=30, 
     xlim=c(-0.3,.6), 
     col=adjustcolor("yellow",alpha.f=0.5), 
     main="A", 
     xlab="Mean SS Index",
     cex.axis=2,
     cex.lab=1.7
     )
#Add the mean of the stranding data
abline(v=mean(allgray$SS.scale), col="blue", lwd=5)
#Add the p-value as text
text(.3,800, "p<0.001", cex=2 )
#This is the formula to calculate the p-value
#(1+sum(xbar.n.SS >= mean(allgray$SS.scale)))/(n+1)

#Repeat above code for RF
hist(xbar.n.RF, 
     breaks=30, 
     xlim=c(-0.4,.6), 
     col=adjustcolor("yellow",alpha.f=0.5), 
     main="B", 
     xlab="Mean RF Index",
     ylab="",
     cex.axis=2,
     cex.lab=1.7
     )
abline(v=mean(allgray$RF.scale), col="blue", lwd=5)
text(.3,800, "p<.0001", cex=2 )
#(1+sum(xbar.n.RF >= mean(allgray$RF.scale)))/(n+1)

#Repeat above code for Ap (here called K)
hist(xbar.n.k, 
     breaks=30, 
     xlim=c(-0.4,.6), 
     col=adjustcolor("yellow",alpha.f=0.5), 
     main="C", 
     xlab="Mean AP Index",
     ylab="",
     cex.axis=2,
     cex.lab=1.7
     )
abline(v=mean(allgray$AP.scale), col="blue", lwd=5)
text(.3,800, "p=0.6", cex=2 )
(1+sum(xbar.n.k >= mean(allgray$AP.scale)))/(n+1)
```

#Autocorrelation Analysis: Figure S1

```{r}
#Order the data from Oldest-Newest
allgray.t=allgray[order(as.Date(allgray$DateAsDate, format = "%m/%d/%Y")),]

#Format the dates so that R can recognize them
allgray.t$DateAsDate=as.Date(allgray.t$DateAsDate, format = "%m/%d/%Y")

#Create an empty column called "Timebetween" and fill it with the number "800" (arbitrarily chosen to be much larger than any of our data)
allgray.t$timebetween=rep(800,nrow(allgray.t))

#Create a function "Find time" which finds the number of days between a stranding, and the most recent previous stranding
findtime=function(data)
{
  #Start with the second data point
  for (i in 2: nrow(data))
{
    #For every data point, takes the curent date subtracts that from the date of the previous data point. Saves the result in the column "timebetween"
data$timebetween[i]=data$DateAsDate[i]-data$DateAsDate[i-1]
  }
data  
}


#Run the function
allgray.t=findtime(allgray.t)
#delete the first entry using the randomly chosen number 800
allgray.t=allgray.t[allgray.t$timebetween<800,]

#Make a histogram of the data with the desired labels
hist(allgray.t$timebetween, 
     xlab="Days between strandings", 
     main="",
     breaks=80,
     xlim=c(0,700),
     col=adjustcolor("gray", alpha.f = 0.5))

#Output the parameters of this data (median, mean, and sd)
median(allgray.t$timebetween)
mean(allgray.t$timebetween)
sd(allgray.t$timebetween)
```

###Lump together all data that occurred within 7, 14, or 21 days of one another

```{r}
n=7 # This is for one week (7). change this variable to 14, or 21 to see results for longer time periods (2 weeks, 3 weeks)

for (i in 1:n)
{
#Run the function from above  
allgray.t=findtime(allgray.t)
#If the number is less than n, remove it from the data
allgray.t=allgray.t[allgray.t$timebetween>i,]
}
#delete the first entry using the randomly chosen number 800
allgray.t=allgray.t[allgray.t$timebetween<800,]

#Plot to make certain it ran correctly
#hist(allgray.t$timebetween, xlab="Days between strandings", main="Stranding Autocorrelation", breaks=80)
```

```{r, echo=FALSE}
#This chunk is to check that the results for RF have not changed using the new data. Output is currently suppressed.
#Rerun the analysis done for the null distribution for RF only

n=10000

xbar.n.RF=rep(0,n)

nulldata.RF.season=nulldata.RF[nulldata.RF$DateAsCont>50,]
nulldata.RF.season=nulldata.RF.season[nulldata.RF.season$DateAsCont<200,]

pb <- winProgressBar(title = "progress bar", min = 0,
                     max = n, width = 300)
for (i in seq(1,n)) {
  
  #Create data
  datause=nulldata.RF.season[sample(nrow(nulldata.RF.season),nrow(allgray.t), replace=TRUE ),]
  #Save the means of each dist
  xbar.n.RF[i]=mean(datause$RF.scale)

  setWinProgressBar(pb, i, title=paste( round(i/n*100, 0),"% done"))
}
close(pb)


hist(xbar.n.RF, 
     breaks=30, 
     xlim=c(-0.4,.6), 
     col=adjustcolor("yellow",alpha.f=0.5), 
     main="strandings > 7 days apart", 
     xlab="Mean RF Index",
     ylab="",
     cex.axis=1.5,
     cex.lab=1.5
     )
abline(v=mean(allgray.t$RF.scale), col="blue", lwd=5)
(1+sum(xbar.n.RF >= mean(allgray.t$RF.scale)))/(n+1)

```


#Test for Multicolinearity

```{r}
library("car")  #Load package
GrayData=create(allgray, nulldata) #Create a dataset
#Make the full model
m1=glm(Stranded~PDO.scale+DateAsCont+RF.scale+AP.scale, data=GrayData, family=binomial)
#VIF table
vif(m1)
#Variance-Covariance Matrix
m2=cov2cor(vcov(m1))
m2=m2[-1,-1]
m2
```

#Table S1: AIC model Selection


 AIC With bootstrapping

```{r}
#This loop makes a logistic model of each permutation of variables from the full model
##It then saves the AIC output from each 
##And bootstraps n times

n=100  #How many iterations?

#Note: Ap=K
S=rep(0,n)       #Season
K=rep(0,n)       #K
RF=rep(0,n)      #RF
PDO=rep(0,n)     #PDO

S_K=rep(0,n)     #Season+K
S_RF=rep(0,n)    #Season+RF
PDO_S=rep(0,n)   #PDO+Season
K_RF=rep(0,n)    #K+RF
PDO_K=rep(0,n)   #K+PDO
PDO_RF=rep(0,n)  #PDO+RF

S_K_RF=rep(0,n)  #Season+K+RF
PDO_S_K=rep(0,n) #Season+K+PDO
PDO_S_RF=rep(0,n)#Season+RF+PDO
PDO_RF_K=rep(0,n)#Season+RF+K

PDO_S_RF_K=rep(0,n)#Season+K+RF+PDO

#Open progress bar
pb <- winProgressBar(title = "progress bar", min = 0,
                     max = n, width = 300)


for (i in seq(1,n)) 
{
  #Create the data
  GrayData=create(allgray, nulldata)
  
  #For each possible permutation of model, create a logistic model and save the AIC value
  
#Season
S[i]=extractAIC(glm(Stranded~DateAsCont, data=GrayData, family=binomial))[2]
#DateAsCont+SS
S_K[i]=extractAIC(glm(Stranded~DateAsCont+AP.scale, data=GrayData, family=binomial))[2]
#SS
K[i]=extractAIC(glm(Stranded~AP.scale, data=GrayData, family=binomial))[2]
#Season+RF
S_RF[i]=extractAIC(glm(Stranded~DateAsCont+RF.scale, data=GrayData, family=binomial))[2]
#RF
RF[i]=extractAIC(glm(Stranded~RF.scale, data=GrayData, family=binomial))[2]
#SS+RF
K_RF[i]=extractAIC(glm(Stranded~AP.scale+RF.scale, data=GrayData, family=binomial))[2]
#Season+SS+RF
S_K_RF[i]=extractAIC(glm(Stranded~DateAsCont+AP.scale+RF.scale, data=GrayData, family=binomial))[2]
#PDO
PDO[i]=extractAIC(glm(Stranded~PDO.scale, data=GrayData, family=binomial))[2]
#PDO+Season
PDO_S[i]=extractAIC(glm(Stranded~PDO.scale+DateAsCont, data=GrayData, family=binomial))[2]
#PDO+K
PDO_K[i]=extractAIC(glm(Stranded~PDO.scale+AP.scale, data=GrayData, family=binomial))[2]
#PDO+RF
PDO_RF[i]=extractAIC(glm(Stranded~PDO.scale+RF.scale, data=GrayData, family=binomial))[2]
#PDO+RF+K
PDO_RF_K[i]=extractAIC(glm(Stranded~PDO.scale+RF.scale+AP.scale, data=GrayData, family=binomial))[2]
#PDO+Season+K
PDO_S_K[i]=extractAIC(glm(Stranded~PDO.scale+DateAsCont+AP.scale, data=GrayData, family=binomial))[2]
#PDO+Season+RF
PDO_S_RF[i]=extractAIC(glm(Stranded~PDO.scale+RF.scale, data=GrayData, family=binomial))[2]
#PDO+Season+RF+K
PDO_S_RF_K[i]=extractAIC(glm(Stranded~PDO.scale+DateAsCont+RF.scale+AP.scale, data=GrayData, family=binomial))[2]

#Update progress bar
setWinProgressBar(pb, i, title=paste( round(i/n*100, 0),
                                        "% done"))
}
#Close progress bar
close(pb)

#Create a list of names for each model
name.AIC=c("Season", "Season, AP", "Ap", "Season, RF", "RF", "AP, RF", "Season, Ap, RF", "PDO", "PDO,Season", "PDO,Ap","PDO,K", "PDO,RF","PDO,Season,Ap", "PDO, Season,RF","PDO,Season,RF")
#Take the mean of each model and round it to the first decimal
geomMean=c(round(mean(S), 1),round(mean(S_K), 1),round(mean(K), 1), round(mean(S_RF), 1), round(mean(RF), 1), round(mean(K_RF), 1), round(mean(S_K_RF), 1), round(mean(PDO),1), round(mean(PDO_S),1), round(mean(PDO_K),1), round(mean(PDO_RF),1), round(mean(PDO_RF_K),1), round(mean(PDO_S_K),1), round(mean(PDO_S_RF),1), round(mean(PDO_S_RF_K),1) )

#Find the smallest AIC value
zero=min(geomMean)
#Calculate the delta AIC for each datapoint
delta=(geomMean-zero)
#Make a table of the name+The AIC+the DeltaAIC
table1=data.frame(t(rbind(name.AIC,geomMean,round(delta,4))))
#Title the columns
names(table1)=c("Model", "AIC", "Delta")
#Order it from smallest AIC to largest
table1[order(table1$AIC),]
```


###Examine the top three models


```{r}
#Model the top three models from the AIC bootstrap and output the summary and marginal effects

#load package
library("margins")
#create data
 GrayData=create(allgray, nulldata)
 
 
#Model the top model
x1 <- glm(Stranded~DateAsCont+RF.scale, data=GrayData, family=binomial)
#Find the marginal effects
m1 <- margins(x1)
#Summarize the model and marginal effects
summary(m1)
summary(x1)

#Repeat for the second model
x2 <- glm(Stranded~DateAsCont+RF.scale+PDO.scale, data=GrayData, family=binomial)
m2 <- margins(x2)
summary(m2)
summary(x2)

#Repeat for the third model
x3 <- glm(Stranded~DateAsCont+RF.scale+AP.scale, data=GrayData, family=binomial)
m3 <- margins(x3)
summary(m3)
summary(x3)

```

#Odds Ratio (Effect Size)

```{r}
#This for-loop makes a model of strandings~SS, and strandings~RF, and saves the beta coefficient
##It then repeats n times

#Initiate variables
n=1000 #Number of times to run the loop

#Place to save the beta coefficient for each model
oddrat_ss=rep(0,n)
oddrat_RF=rep(0,n)

for (i in 1:n)
{
  #Create data
GrayData=create(allgray, nulldata)

#Glm of SS
myglm=glm(Stranded~MaxSS, data=GrayData, family=binomial)
#Save exp(beta coeff), or the odds ratio
oddrat_ss[i]=exp(coef(myglm))[2]

#Glm of RF
myglm2=glm(Stranded~RadioFluxLA, data=GrayData, family=binomial)
#Save exp(beta coeff), or the odds ratio
oddrat_RF[i]=exp(coef(myglm2))[2]
}

#Find the mean odd ratio for each model
mean(oddrat_ss)
mean(oddrat_RF)

#Calculate the likelihood increase from the smallest to largest bin from the visualization section
mean(oddrat_ss)^(151)
mean(oddrat_RF)^(165)
```
